## Netty权威指南

### linux网络io模型
 * 阻塞io模型
 * 非阻塞io模型
 * io多路复用模型
   + 把多个io阻塞复用到同一个select的阻塞上，从而使用单线程情况下可以同时处理多个客户端请求。
   + select/poll顺序扫描df就绪状态类型
   + epoll 事件监听类型
 * 非阻塞io多路复用模型（信号驱动io模型）
 * 异步io模型
   
#### io多路利用技术
 * 把多个io阻塞复用到同一个select的阻塞上，从而使用单线程情况下可以同时处理多个客户端请求。
 * 应用场景
   + 服务器同时处理多个处于监听状态或者多个连接状态的套接字；
   + 服务器同时处理多种网络协议的套接字；
 * linux网络编程
   + select 有缺陷不再使用, 使用epoll替代
 * select/poll 
   + 单进程找开的FD数量有限制，由FD_SETSIZE设置，默认1024个。
   + 对于那些需要支持上万个TCP连接的大型服务器来说显然太少了。
   + 可以选择修改这个宏然后重新编译内核，不过这会带来网络效率的下降。
   + 可以通过多进程方案解决， 但进程间交换数据非常麻烦。java进程间没有共享内存，实现数据同步更加复杂。
   + 当拥有一个很大的socket集合，由于网络延时或链路空闲，任一时刻只有少部socket是活跃的，但select/poll每次调用都会线性扫描全部集合，导致效率线性下降。
 * epoll
   1. 支持进程打开的socket描述符(FD)不受限制（公受限于操作系统的最大文件句柄数）  
      + 1G内存的机器上大约有10万个句柄左右。
   2. i/o效率不会随着FD数目的增加而线性下降
      + epoll根据每个fd上的callback函数实现的，只有活动的socket才会调用callback函数
   3. 使用mmap加速内核与用户空间的消息传递
      + select, poll, epoll都需要内核把fd消息通知给用户空间，如何避免不必要的内存复制显得非常重要。
      + epoll通过内核和用户空间mmap同一块内存实现
      
### java Nio
#### 传统BIO编程
 1. 服务器端阻塞监听客户端连接请求
 2. 服务器端为每一个客户端连接socket处理一个线程来处理请求
   + 当请求过大，线程数膨胀之后，系统性能将急剧下降，随着时间推移会导致java栈溢出，创建线程失败，并最终导致进程宕机或者僵死。
   
#### 伪异步I/O编程
 * 通过采用线程池+消息队列来代替BIO编程带来的问题
 * 可以控制线程池大小，消息队列大小。从而避免造成系统资源被耗尽。
 1. 将客户端socket连接封装成任务放到线程池中执行，当线程达到最大后，把连接放到消息队列
   + 由于socket, inputstream, outstream都是阻塞方式进行读写。
   + 对于inputstream，当对方发请求或者应答消息比较缓慢、或者网络传输较慢时，读取输入流一方通信线程将被长时间阻塞
     - 如对方需要60s才能将数据发送完，读取一方IO线程将会被同步阻塞60s。此进其它消息只能在消息队列中排队。
   + 对于outputstream, 当接收方处理缓慢时，将不能及时从TCp缓冲区读取数据，导致发送方TCP windows size不断减少直到0，双方处理Keep-Alive状态，write一直阻塞直到size大于0.
 * 读写都是阻塞的，阻塞的时间取决于对方IO处理速度和网络IO的传输速度。如果程序依赖于对方处理速度，那可靠性就非常差。
   + 如果线程池中线程每个线程都是处理60s, 则其它消息则在消息队列中等待。
   + 如果消息队也满，则消息会被丢掉或入队操作被阻塞
   + 由于入队被阻塞，则接收客户端连接将被阻塞
   + 导致其它连接请求都超时。
   + 客户端认为系统崩溃，无法处理新的请求消息。
   
 
#### java网络开发
 * 自己编写java网络通信框架与使用netty网络通信框架区别
 * 自己编写需要具备知识
   + 熟悉java nio api
   + 熟悉java多线程编程
   + 熟悉网络通信过程
   + 熟悉维护网络健壮性：避免网络错误
     - 一条连接问题：半包读写，网络闪断，断线重连，重复消息，异常码流
     - 多条连接问题：网络拥塞，
   + 注意java nio中的bug, 如epoll bug, 它会导致selector空轮询，最终导致cpu100
   
 * 使用netty框架
   + api简单
   + 成熟，稳定，修复了java nio中的所有bug
   + 支持多种网络协议
   + 通过ChannelHandler实现灵活扩展
   + 功能强大，支持多种io模型
   
### netty

#### tcp粘包与拆包发生的原因
 * 有两个粘包：发送端粘包与接收端粘包
 * 发送端粘包：
   + 启用nagle优化算法时
 * 接收端粘包：
   + 应用程序处理消息不及时，而网络很快
 * 应用程序write写入的字节大小大于套接字发送缓冲区大小, 发生拆包;
 * 进行MSS大小的TCP分段;
 * 以太网帧的(载荷)payload大于MTU进行IP分片;

#### tcp粘包解决策略
 * 消息定长
 * 在包尾增加回车换行进行分割
 * 将消息分为消息头与消息体
 
 
### 编解码技术
#### 编解码框架优劣
 * 是否支持跨语言
 * 编码后码流大小
 * 编解码性能
 * API是否方便使用
 * 手工开发工作量和难度
#### java序列化缺点
 * 不能跨语言
 * 序列化后码流太大
 * 序列化性能太低
 
### 私有协议栈开发
 
#### 链路关闭
 * 消息读写过程中，发生了I/O异常，需要主动关闭
 * 心跳消息读写过程中，发生了I/O异常，需要主动关闭
 * 心跳超时
 * 发生编码异常等不可恢复错误时
 * 消息功击时
 
#### 可靠性设计
 * 心跳机制，采用客户端ping, 服务器端pong响应ping
   + 当客户端未收到N个pong后，断开连接。间隔N时间后发起重连操作。(注意不要马上重连) 
   
 * 重连机制
   + 如果链路中断，等待N时间后，由客户端发起重连接操作，如果重连失败，间隔N后再重连。
   + 为什么要等待N时间后重连，因为要等待服务器端有充足时间释放句柄资源。
 * 重复登录机制
   + 防止客户端异常状态下反复重连导致句柄资源被耗尽。
   + 如果客户端与服务器对链路状态理解不一致导致客户端无法握手成功的问题，
   + 当服务器通过判断心跳超时主动关闭链路，清空客户端数据，以保证后续可以重连成功。
 * 消息缓存重发
   + 无论是客户端还是服务器，当发生链路中断，在链路恢复前，缓存消息队列中等待发送的消息，重连后重发。
   + 考虑内存溢出的风险，建议设置队列大小
 
#### 安全性设计
 * 添加IP白名单
 
#### 扩展性设计

### Netty源码
#### ByteBuf
 * ByteBuffer缺点
   + 长度固定，一旦分配完成，不能改变。
   + 只有一个position指针，操作必须小心
   + API功能有限，一些特性需要自己实现。
   
> 由于NIO的Channel读写的参数都是ByteBuffer，因此Netty的ByteBuf必须提供接口方便将ByteBuf转为ByteBuffer
> 或将ByteBuffer转为ByteBuf, 考虑到性能，应该尽量避免缓冲区的复制
> 内部实现可以考虑聚合一个ByteBuffer

 * MaxCapacity: 最大容量

 * 顺序读写
   + read*
   + write*
     - 如果写入字节数小于MaxCapacity时，会自动扩容。
     - 否则抛出异常
   > 为什么要自动扩容而不是像ByteBuffer抛出异常：
   > 由于很多场景下我们无法预先判断需要编码和解码的POJO对象长度
   > 因引只能跟据经验值给个估计值，如果这个值偏大，就会导致内存浪费
   > 如果偏小，遇到大消息时就会发生缓冲区溢出异常，使用者需要自己捕获异常
   > 并重新计算缓冲区大小，将原来数据复制到新的缓冲区中，然后重置指针。
   > 这种处理策略对用户非常不友好，而且会引入新的问题。
   
 * readIndex, writeIndex
   + 这两个指针将ByteBuf分成三个区域
   + discardable bytes, readable bytes, writable bytes
  
 * discardable bytes
   + 缓冲区的分配与释放是个耗时的操作，因此我们尽量重用它们。
   + 但调用discardReadBytes时会发生字节数组的内存复制，所以频繁调用将会导致性能下降。
   
 * readable bytes, writable bytes
   + 如果读取字节数长度大于实际可读字节数，则抛出异常
   + 如果写入字节数长度大于实际可写字节数，则抛出异常
   
 * clear操作
   + 它并不会清空缓冲区内容本身，只是重置指针
   
 * Mark, rest
   + 分配有read, write mark rest接口
   
 * 查找操作
   + indexof
   + byteBefore
   + foreachByte
   
 * derived buffers : 视图buffer
   + duplicate: 复制对象，共享缓冲区内容
   + slice: 子缓冲区， 共享缓冲区内容
   + copy: 复制对象，不共享缓冲区内容
   
 * 转换成标准的ByteBuffer
   + nioBuffer(): 两者共用一个缓冲区内容引用，返回后ByteBuffer无法感知原ByteBuf的动态扩展
  
 * 随机读写
   + get*
   + set*
     - 不会扩容，写入长度大于可写字节数抛出异常。
     
#### ByteBuf分类
 * 堆内存
   + 内存分配与回收速度快，可以被JVM回收；
   + 缺点：I/O读写时，需要额外做一次内存复制，性能会有一定下降。
 * 直接内存
   + 分配与回收速度慢一些，
   + 优点：I/O读写时，不需要内存复制，速度比堆块。
> 由于两者的优缺点，所以最佳实践是在I/O通信线程的读写缓冲区使用直接内存
> 后端业务的缓解码模块使用堆内存，这样组合可以达到性能最优。

 * 对象池ByteBuf与普通ByteBuf
   + 对象池ByteBuf可以循环利用创建的ByteBuf对象，提升内存利用率，降低由于高负载导致的频繁GC。测试表明在高负载，大并发冲击下内存和GC更加平稳。
   + 但对象池ByteBuf管理和维护更加复杂，使用时更加谨慎。
   
#### ByteBuf扩容
 * 阀值：4M
 * 扩容容量不能超过MaxCapacity
 * 扩容容量：当前writeIndex + minWritableBytes(要写的字节数)
 * 扩容采用两种算法：步进与倍增算法
   + 当扩容容量小于阀值，采用倍增算法，当扩容空间大于阀值，采用步进算法。
   + 先采用倍增算法：初始时内存比较小，倍增操作并不会带来太多的内存浪费，当内存增长到达阀值后，再进行倍增可能会带来额外的内存浪费。
   + 由于每个客户端连接都可能维护自己独立的接收与发送缓存区，这样随着客户读的线性增长，内存浪费会成比例增加，因此到达阀值后采用步进增长。
   + 步进算法：当扩容容量大于阀值，使用。看扩容容量是阀值的几倍，a/b * b, 即a/b算出a是b的几倍，然后乘以b.
   
   
 
   
   